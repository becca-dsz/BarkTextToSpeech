# ğŸ”„ Transformers

## ğŸ—¨ï¸ Language Models
Chatgpt, powered by a model called GPT is based on a deep learning architecture called Transformers. Transformers have sparked a major boom in modern AI because of their capability to understand and generate human-like text. 

## ğŸ—¨ï¸ What is a Language Model?
Language models are machine learning models designed to predict the next word in a sentence, given a sequence of previous words. Examples: Gmail's auto-complete, Google's BERT (Bidrectional Encoder Representations from Transformers), and OpenAI's GPT (Generative Pretrained Transformer). GPT is a large language model due to its billions of parameters and massive training dataset, making it more powerful than models like BERT. Fundamentally, GPT and similar models operate by predicting one word at a time, iterating until a complete and coherent answer or sentence is produced. This next-word prediction ability is at the core of language models.

## ğŸ—£ï¸ Understanding Word Embeddings

Machine learning models donâ€™t understand text directly; they understand numbers (vectors).
Words can be represented numerically using word embeddings, vectors that encode semantic meaning.
Example: â€œKingâ€ represented as a vector with features that correspond to concepts like authority, gender, richness.
Embeddings capture relationships: King - Man + Woman â‰ˆ Queen.
Real embeddings have hundreds (e.g., 300 in Googleâ€™s Word2Vec) or thousands (e.g., 12,000 in GPT) of dimensions.
Meaningful relationships exist in vector space, such as gender or country-capital vectors.

## ğŸ“œ Static VS Contextual Embeddings

Static embeddings: One fixed vector per word regardless of context (e.g., Word2Vec, GloVe).
Static embeddings canâ€™t differentiate word meanings in different contexts (e.g., â€œtrackâ€ as a railway vs. â€œtrackâ€ as to follow).
Contextual embeddings: Word vectors change based on sentence context.
Example of contextual embedding: The word â€œdishâ€ changes meaning in â€œrice dishâ€ vs. â€œcheese dish.â€
Contextual embeddings improve next-word prediction by adjusting based on nearby words and sentence context.

## ğŸ§¬ Generating Conceptual Embeddings using Transformers

Contextual embedding is generated by combining static embeddings with influence vectors for modifiers and adjectives.
The process can be visualized as adding vectors representing modifiers to the base vector (e.g., adding â€œIndian,â€ â€œsweetâ€ to â€œrice dishâ€ embedding).
Transformers create rich contextual embeddings by weighing all wordsâ€™ influence on each other in a sentence.

## ğŸ“¥ Transformer Architecture: Encoding and Decoding

Transformer architecture comprises:
Encoder: Processes input data and produces contextual embeddings for each token.
Decoder: Uses the encoderâ€™s intput to generate output sequence (like predicting next words or translating).
Example: English to Hindi translationâ€”encoder produces embeddings for English sentence; decoder generates Hindi sentence word-by-word.
BERT uses only the encoder part.
GPT uses only the decoder part, but still follows the Transformer architecture principles.

## ğŸ§  Training VS Inference

In training, the model learns from large datasets by predicting next words and minimizing errors using backpropagation.
During inference, the trained model predicts next words or translates sentences.
Model training uses massive text corpora (Wikipedia, internet, books).
Probability distribution over vocabulary is learned to predict the most likely next word.

## ğŸ”¡ Vocabulary and Tokenization
Vocabulary consists of â€œtokensâ€ rather than perfect words.
Tokens can be sub-word units (e.g., â€œplayingâ€ can be tokenized into â€œplayâ€ + â€œingâ€).
BERT uses ~30,000 tokens, GPT uses ~50,000 tokens.
Each token has a static embedding vector stored in a matrix (size varies by model, e.g., 768 for BERT, 12,228 for GPT).

## ğŸ”¹ Tokenization and Positional Embeddings 
After tokenizing input, static embeddings are retrieved for each token from the embedding matrix.
Transformers process entire sequences in parallel, not sequentially like RNNs.
Since word order is important, positional embeddings are added to static embeddings.
Positional embeddings encode word order information so the model knows the position of each token.
Positional embeddings are generated via a sine/cosine formula from the original Transformer paper.

## ğŸ‘€ Attention Mechanism
Attention allows words to â€œattendâ€ to other relevant words when encoding meaning.
Example: For the word â€œIndian,â€ attention looks at related words like â€œdosa,â€ â€œdola,â€ and pronouns like â€œBâ€ in the sentence.
Attention assigns weights to each word showing how much it influences the target wordâ€™s meaning (e.g., 36% sweet, 14% Indian, etc.).
Attention scores are normalized probabilities showing the level of relevance.

## ğŸ”‘ Query, Key, and Value Vectors: Intuitive Explanation
Query-Key-Value concept explained via library and professor-student analogies:
Query: What you are searching for or interested in (e.g., â€œI want quantum physics booksâ€).
Key: Metadata or descriptors used to index and locate relevant data (e.g., book labels or studentsâ€™ expertise).
Value: The actual content or data retrieved (e.g., the book or essay content).
<br>
In Transformers:
Query, Key, and Value vectors are derived from token embeddings.
Attention weights are computed as dot products of Query and Key vectors, passed through a softmax to obtain probabilities.
Values are weighted sums combining the most relevant information for contextual embedding.
This modifies static embeddings into rich, context-aware embeddings.

