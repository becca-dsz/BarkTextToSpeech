# ğŸ”„ Transformers

## ğŸ—¨ï¸ Language Models
Chatgpt, powered by a model called GPT is based on a deep learning architecture called Transformers. Transformers have sparked a major boom in modern AI because of their capability to understand and generate human-like text. 

## ğŸ—¨ï¸ What is a Language Model?
Language models are machine learning models designed to predict the next word in a sentence, given a sequence of previous words. Examples: Gmail's auto-complete, Google's BERT (Bidrectional Encoder Representations from Transformers), and OpenAI's GPT (Generative Pretrained Transformer). GPT is a large language model due to its billions of parameters and massive training dataset, making it more powerful than models like BERT. Fundamentally, GPT and similar models operate by predicting one word at a time, iterating until a complete and coherent answer or sentence is produced. This next-word prediction ability is at the core of language models.

## ğŸ—£ï¸ Understanding Word Embeddings

Machine learning models donâ€™t understand text directly; they understand numbers (vectors).
Words can be represented numerically using word embeddings, vectors that encode semantic meaning.
Example: â€œKingâ€ represented as a vector with features that correspond to concepts like authority, gender, richness.
Embeddings capture relationships: King - Man + Woman â‰ˆ Queen.
Real embeddings have hundreds (e.g., 300 in Googleâ€™s Word2Vec) or thousands (e.g., 12,000 in GPT) of dimensions.
Meaningful relationships exist in vector space, such as gender or country-capital vectors.

## ğŸ“œ Static VS Contextual Embeddings

Static embeddings: One fixed vector per word regardless of context (e.g., Word2Vec, GloVe).
Static embeddings canâ€™t differentiate word meanings in different contexts (e.g., â€œtrackâ€ as a railway vs. â€œtrackâ€ as to follow).
Contextual embeddings: Word vectors change based on sentence context.
Example of contextual embedding: The word â€œdishâ€ changes meaning in â€œrice dishâ€ vs. â€œcheese dish.â€
Contextual embeddings improve next-word prediction by adjusting based on nearby words and sentence context.

## ğŸ§¬ Generating Conceptual Embeddings using Transformers

Contextual embedding is generated by combining static embeddings with influence vectors for modifiers and adjectives.
The process can be visualized as adding vectors representing modifiers to the base vector (e.g., adding â€œIndian,â€ â€œsweetâ€ to â€œrice dishâ€ embedding).
Transformers create rich contextual embeddings by weighing all wordsâ€™ influence on each other in a sentence.

## ğŸ“¥ Transformer Architecture: Encoding and Decoding

Transformer architecture comprises:
Encoder: Processes input data and produces contextual embeddings for each token.
Decoder: Uses the encoderâ€™s intput to generate output sequence (like predicting next words or translating).
Example: English to Hindi translationâ€”encoder produces embeddings for English sentence; decoder generates Hindi sentence word-by-word.
BERT uses only the encoder part.
GPT uses only the decoder part, but still follows the Transformer architecture principles.

## ğŸ§  Training VS Inference

In training, the model learns from large datasets by predicting next words and minimizing errors using backpropagation.
During inference, the trained model predicts next words or translates sentences.
Model training uses massive text corpora (Wikipedia, internet, books).
Probability distribution over vocabulary is learned to predict the most likely next word.